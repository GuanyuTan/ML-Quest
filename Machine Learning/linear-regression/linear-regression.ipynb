{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "[LECTURE NOTES HERE](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote08.html)\n",
    "\n",
    "Given real valued input vector $\\vec{x}$, predict real valued output $y$.\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "1. Each data point $\\vec{x}$ is linearly correlated with each output $y$.\n",
    "2. Observations are noisy. For each $\\vec{x}$ are i.i.d drawn from its respective Gaussian distribution centered on a point on the linear regression line, or formally:\n",
    "    \n",
    "    $y_i = w^Tx_i+\\epsilon_i$, $\\epsilon_i\\backsim \\mathcal{N}(0, \\sigma)$\n",
    "    or\n",
    "    \n",
    "    $y_i \\backsim \\mathcal{N}(w^Tx_i, \\sigma)$\n",
    "\n",
    "Using MLE, we get :\n",
    "\n",
    "$argmin_w \\frac{1}{n} \\sum_{i=1}^n(x_i^Tw-y_1)^2$\n",
    "\n",
    "Using MAP, we get :\n",
    "\n",
    "$argmin_w \\frac{1}{n} \\sum_{i=1}^n(x_i^Tw-y_1)^2 + \\lambda||w||^2_2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "x,y = make_regression(n_features=10)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        m,n = X.shape\n",
    "        self.weights = np.ones((n+1))\n",
    "        self.alpha = 0.1\n",
    "        X = np.hstack((X, np.ones(m).reshape((m,1))))\n",
    "        # weights include bias\n",
    "        for i in range(15000):\n",
    "            y_hat = X.dot(self.weights)\n",
    "            # batch gradient descent\n",
    "            self.weights = self.weights+self.alpha*np.mean(X*(y-y_hat).reshape(-1,1),axis=0)\n",
    "    \n",
    "    def cost(self, y_hat, y):\n",
    "        return np.mean(np.power(y-y_hat,2))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        m,n = X.shape\n",
    "        X = np.hstack((X, np.ones(m).reshape((m,1))))\n",
    "        return X.dot(self.weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.385538569718912e-26"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "out = model.predict(x_test)\n",
    "mean_squared_error(out, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.674995775168242e-26\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "out = model.predict(x_test)\n",
    "print(mean_squared_error(out, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
